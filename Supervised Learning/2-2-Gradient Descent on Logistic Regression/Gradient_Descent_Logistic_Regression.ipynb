{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent - Logistic Regression\n",
    "\n",
    "## Introduction\n",
    "In this Jupyter notebook, we apply gradient descent to optimize a logistic regression model for the classic Titanic dataset. \n",
    "We aim to predict passenger survival based on various features like ticket class, sex, age, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Overview\n",
    "\n",
    "Gradient descent logistic regression is an optimization algorithm that refines the parameters of a logistic regression model, which is commonly used for binary classification problems. The logistic regression model predicts the probability of a binary outcome based on one or more predictor variables. Here are the key steps in this algorithm:\n",
    "\n",
    "### Model Definition\n",
    "We begin by defining the logistic regression model, which estimates the probability $P(Y=1|X)$ of the target variable $Y$ being in the default class (\"1\") given the input features $X$. The logistic function used for the prediction is given by:\n",
    "\n",
    "$$ P(Y=1|X) = \\frac{1}{1 + e^{-(b_0 + b_1X)}} $$\n",
    "\n",
    "where:\n",
    "- $b_0$ is the y-intercept or bias term.\n",
    "- $b_1$ represents the coefficient for the input feature $X$.\n",
    "- $e$ is the base of the natural logarithm.\n",
    "\n",
    "The output of this function is a probability that ranges between 0 and 1.\n",
    "\n",
    "### Cost Function\n",
    "To measure how well our model fits the training data, we use the binary cross-entropy loss or log loss as the cost function:\n",
    "\n",
    "$$ J(b_0, b_1) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(P(y^{(i)}|x^{(i)})) + (1 - y^{(i)}) \\log(1 - P(y^{(i)}|x^{(i)})) \\right] $$\n",
    "\n",
    "where:\n",
    "- $m$ is the number of training examples.\n",
    "- $y^{(i)}$ is the actual label for the $i$-th training example.\n",
    "- $P(y^{(i)}|x^{(i)})$ is the model's predicted probability that the $i$-th sample belongs to class 1.\n",
    "\n",
    "### Gradient Descent\n",
    "Gradient descent is a core part of the training process that involves iteratively adjusting the parameters $b_0$ and $b_1$ to minimize the cost function. This is done using the update rules:\n",
    "\n",
    "$$ b_0 := b_0 - \\alpha \\frac{\\partial J}{\\partial b_0} $$\n",
    "$$ b_1 := b_1 - \\alpha \\frac{\\partial J}{\\partial b_1} $$\n",
    "\n",
    "where $\\alpha$ is the learning rate, which controls the size of the steps we take towards the minimum of the cost function.\n",
    "\n",
    "### Partial Derivatives\n",
    "The gradients of the cost function with respect to the model parameters are computed as follows:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial b_0} = \\frac{1}{m} \\sum_{i=1}^{m} (P(y^{(i)}|x^{(i)}) - y^{(i)}) $$\n",
    "$$ \\frac{\\partial J}{\\partial b_1} = \\frac{1}{m} \\sum_{i=1}^{m} (P(y^{(i)}|x^{(i)}) - y^{(i)}) x^{(i)} $$\n",
    "\n",
    "These partial derivatives represent the slopes of the cost function at any point and direct the update step towards the minimum.\n",
    "\n",
    "### Updating the Parameters\n",
    "We apply the update rules repeatedly for a number of iterations or until the changes to $b_0$ and $b_1$ are below a certain threshold, indicating convergence.\n",
    "\n",
    "### Prediction\n",
    "Once the model parameters are optimized, for a new set of input features, the model predicts the binary outcome (1 or 0). The decision rule is:\n",
    "\n",
    "- If $P(Y=1|X) > 0.5$, predict class \"1\".\n",
    "- If $P(Y=1|X) \\leq 0.5$, predict class \"0\".\n",
    "\n",
    "### Limitations and Assumptions\n",
    "While gradient descent logistic regression is effective for binary classification tasks, it assumes a linear relationship between the log odds of the outcome and the input features. The model can also be sensitive to outliers, which can disproportionately influence the position of the decision boundary.\n",
    "\n",
    "This detailed explanation outlines the theoretical underpinnings of logistic regression optimized by gradient descent, providing a foundation for its application to practical classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "Below, we load the Titanic dataset, preprocess it, and apply logistic regression using gradient descent to predict survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass   age  sibsp  parch     fare  adult_male  alone  sex_male  \\\n",
      "0         0       3  22.0      1      0   7.2500        True  False         1   \n",
      "1         1       1  38.0      1      0  71.2833       False  False         0   \n",
      "2         1       3  26.0      0      0   7.9250       False   True         0   \n",
      "3         1       1  35.0      1      0  53.1000       False  False         0   \n",
      "4         0       3  35.0      0      0   8.0500        True   True         1   \n",
      "\n",
      "   embarked_Q  embarked_S  class_Second  class_Third  who_man  who_woman  \n",
      "0           0           1             0            1        1          0  \n",
      "1           0           0             0            0        0          1  \n",
      "2           0           1             0            1        0          1  \n",
      "3           0           1             0            0        0          1  \n",
      "4           0           1             0            1        1          0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = sns.load_dataset('titanic')\n",
    "\n",
    "# Drop columns with high missing values and irrelevant information\n",
    "df.drop(columns=['deck', 'embark_town', 'alive'], inplace=True)\n",
    "\n",
    "# Convert categorical variables to dummy variables\n",
    "df = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "# Handle missing values by imputation\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Display the first few rows of the processed dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(max_iter=1000, solver='saga'))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Prepare features and labels\n",
    "X = df.drop('survived', axis=1)\n",
    "y = df['survived']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression pipeline with preprocessing\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(solver='saga', max_iter=1000))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8156424581005587\n",
      "Confusion Matrix:\n",
      "[[92 13]\n",
      " [20 54]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       105\n",
      "           1       0.81      0.73      0.77        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.80      0.81       179\n",
      "weighted avg       0.82      0.82      0.81       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Classification Report:\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8156424581005587\n",
      "Confusion Matrix:\n",
      "[[92 13]\n",
      " [20 54]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.88      0.85       105\n",
      "           1       0.81      0.73      0.77        74\n",
      "\n",
      "    accuracy                           0.82       179\n",
      "   macro avg       0.81      0.80      0.81       179\n",
      "weighted avg       0.82      0.82      0.81       179\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'pipeline' is already fitted and 'X_test', 'y_test' are defined\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Plotting the confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Model Performance\n",
    "Our logistic regression model achieved an accuracy of approximately 81.56% on the test dataset. This is a robust starting point, considering the simplicity of the model and the complexity of human survival factors during the Titanic disaster.\n",
    "\n",
    "### Confusion Matrix Insights\n",
    "The confusion matrix visualized above offers deeper insights:\n",
    "\n",
    "- True Negative (TN): 92 passengers were correctly predicted not to survive.\n",
    "- False Positive (FP): 13 passengers were incorrectly predicted to survive.\n",
    "- False Negative (FN): 20 passengers were incorrectly predicted not to survive.\n",
    "- True Positive (TP): 54 passengers were correctly predicted to survive.\n",
    "\n",
    "The model shows a higher number of true negatives than true positives, indicating a slightly better performance in predicting non-survival. The lower number of false negatives compared to false positives suggests that the model is more conservative in predicting survival, potentially an artifact of the underlying class distribution.\n",
    "\n",
    "### Classification Report Analysis\n",
    "The classification report gives us the following metrics:\n",
    "\n",
    "- **Precision for Non-Survivors (0)**: Out of all predictions for non-survival, 82% were correct.\n",
    "- **Recall for Non-Survivors (0)**: Out of all actual non-survivors, the model correctly identified 88%.\n",
    "- **F1-Score for Non-Survivors (0)**: The F1-score, a balance between precision and recall, is 85% for non-survivors.\n",
    "\n",
    "Similarly, for survivors (1):\n",
    "\n",
    "- **Precision for Survivors (1)**: Out of all predictions for survival, 81% were correct.\n",
    "- **Recall for Survivors (1)**: The model correctly identified 73% of all actual survivors.\n",
    "- **F1-Score for Survivors (1)**: The F1-score for survivors is 77%.\n",
    "\n",
    "The model demonstrates a balanced performance across both classes with a slight inclination towards correctly identifying non-survivors.\n",
    "\n",
    "### Implications and Future Work\n",
    "This model serves as a preliminary step in predictive analytics for binary classification problems. It is apparent that there is room for improvement, especially in reducing the number of false positives and false negatives. Future work might involve:\n",
    "\n",
    "- Feature engineering to better capture the underlying patterns in the data.\n",
    "- Hyperparameter tuning of the logistic regression model.\n",
    "- Experimenting with more complex models like Random Forests or Gradient Boosted Trees.\n",
    "- Applying class-weight adjustments to handle imbalances in the dataset.\n",
    "- Utilizing cross-validation to ensure the model's generalizability to new data.\n",
    "\n",
    "In conclusion, the results obtained provide valuable predictions about survival on the Titanic, showcasing the power of logistic regression when combined with gradient descent optimization. With further refinement, the predictive performance can likely be improved, leading to more accurate and insightful results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
